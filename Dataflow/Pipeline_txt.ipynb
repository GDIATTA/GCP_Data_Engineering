{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Beam\n",
    "**Apache Beam** Apache Beam is a library for data processing. It is often used for Extract-Transform-Load (ETL) jobs, where we:<br>\n",
    "> Extract from a data source <br>\n",
    "> Transform that data <br>\n",
    "> Load that data into a data sink (like a database) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this tutorial, we need only to import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **SMSSpamCollection** is the file that we will use in this tutorial. This file contains the informations mail about **Spam** and **Ham**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In PCollection**, Read a dataset from a txt File <br>\n",
    "> **In PTransformation**, split each element of the PCollection by tab and putting them into a list <br>\n",
    "> **In Writing on file**, write the results into a file.txt <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output1-00000-of-00001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x180cc0dbb50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a pipeline and assign it by naming **pipeline1**\n",
    "pipeline1 = beam.Pipeline()\n",
    "outputs = (\n",
    "    pipeline1\n",
    "    | \"Read a dataset\" >> beam.io.ReadFromText('SMSSpamCollection')\n",
    "    # ADDED a Map\n",
    "    | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
    "    | 'Write results' >> beam.io.WriteToText(\"C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output1\")\n",
    "    | 'Print the text file name' >> beam.Map(print) # or beam.LogElements()\n",
    "\n",
    ")\n",
    "# To run the pipeline\n",
    "pipeline1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In PCollection**, Read a dataset from a txt File <br>\n",
    "> **In PTransformation**, split each element of the PCollection by tab and putting them into a list <br>\n",
    "> **In PTransformation**, transform to only return a PCollection that only contains lists with the label **spam** <br>\n",
    "> **In Writing on file**, write the results into a file.txt <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output2-00000-of-00001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x180cc0da010>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a pipeline and assign it by naming **pipeline2**\n",
    "pipeline2 = beam.Pipeline()\n",
    "outputs = (\n",
    "    pipeline2\n",
    "    | \"Read a dataset\" >> beam.io.ReadFromText('SMSSpamCollection')\n",
    "    # ADDED a Map\n",
    "    | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
    "     # ADDED Filter\n",
    "    | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
    "    | 'Write results' >> beam.io.WriteToText(\"C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output2\")\n",
    "    | 'Print the text file name' >> beam.Map(print) # or beam.LogElements()\n",
    "\n",
    ")\n",
    "# To run the pipeline\n",
    "pipeline2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. FlatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In PCollection**, Read a dataset from a txt File <br>\n",
    "> **In PTransformation**, split each element of the PCollection by tab and putting them into a list <br>\n",
    "> **In PTransformation**, transform to only return a PCollection that only contains lists with the label **spam** <br>\n",
    "> **In PTransformation**,  transform that takes in the function **lambda line: re.findall(r\"[a-zA-Z']+\", line[1])** to your code below <br>\n",
    "> **In Writing on file**, write the results into a file.txt <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output3-00000-of-00001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x180cc5cda50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a pipeline and assign it by naming **pipeline3**\n",
    "pipeline3 = beam.Pipeline()\n",
    "outputs = (\n",
    "    pipeline3\n",
    "    | \"Read a dataset\" >> beam.io.ReadFromText('SMSSpamCollection')\n",
    "    # ADDED a Map\n",
    "    | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
    "     # ADDED Filter\n",
    "    | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
    "    # ADDED FlatMap\n",
    "    | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
    "    | 'Write results' >> beam.io.WriteToText(\"C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output3\")\n",
    "    | 'Print the text file name' >> beam.Map(print) # or beam.LogElements()\n",
    "\n",
    ")\n",
    "# To run the pipeline\n",
    "pipeline3.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In PCollection**, Read a dataset from a txt File <br>\n",
    "> **In PTransformation**, split each element of the PCollection by tab and putting them into a list <br>\n",
    "> **In PTransformation**, transform to only return a PCollection that only contains lists with the label **spam** <br>\n",
    "> **In PTransformation**,  transform that takes in the function **lambda line: re.findall(r\"[a-zA-Z']+\", line[1])** to your code below <br>\n",
    "> **In PTransformation**, associate each word with a numerical value 1 by using **Map(lambda word: (word, 1))** to the pipeline <br>\n",
    "> **In PTransformation**, combine these numerical values to sum up all the counts of each word <br>\n",
    "> **In Writing on file**, write the results into a file.txt <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output4-00000-of-00001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x180cc495110>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a pipeline and assign it by naming **pipeline4**\n",
    "pipeline4 = beam.Pipeline()\n",
    "outputs = (\n",
    "    pipeline4\n",
    "    | \"Read a dataset\" >> beam.io.ReadFromText('SMSSpamCollection')\n",
    "    # ADDED a Map\n",
    "    | 'Separate to list' >> beam.Map(lambda line: line.split(\"\\t\"))\n",
    "     # ADDED Filter\n",
    "    | 'Keep only spam' >> beam.Filter(lambda line: line[0] == \"spam\")\n",
    "    # ADDED FlatMap\n",
    "    | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line[1]))\n",
    "    | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
    "    # ADDED CombinePerKey\n",
    "    | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "    | 'Write results' >> beam.io.WriteToText(\"C:/Users/Lenovo/OneDrive/Documents1/Apache_Beam/Project2/output4\")\n",
    "    | 'Print the text file name' >> beam.Map(print) # or beam.LogElements()\n",
    "\n",
    ")\n",
    "# To run the pipeline\n",
    "pipeline4.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
